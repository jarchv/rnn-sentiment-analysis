{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142â€“150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively.\n",
    "For simplicity, I assembled the reviews in a single CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shuffle the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"Murder in Greenwich\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available',\n",
       "       1], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conform that the `stream_docs` function fetches the documents as intended, let us execute the following code snippet before we implement the `get_minibatch` function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we confirmed that our `stream_docs` functions works, we will now implement a `get_minibatch` function to fetch a specified number (`size`) of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        docs.append(text)\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define a simple `tokenizer` that splits the text into individual word tokens. Furthermore, we will use some simple regular expression to remove HTML markup and all non-letter characters but \"emoticons,\" convert the text to lower case, remove stopwords, and apply the Porter stemming algorithm to convert the words into their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    text = re.sub(r\"it's\", \" it is\", text)\n",
    "    text = re.sub(r\"that's\", \" that is\", text)\n",
    "    text = re.sub(r\"\\'s\", \" 's\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"won't\", \" will not\", text)\n",
    "    text = re.sub(r\"don't\", \" do not\", text)\n",
    "    text = re.sub(r\"can't\", \" can not\", text)\n",
    "    text = re.sub(r\"cannot\", \" can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "\n",
    "    text = re.sub('[\\W]+', ' ', text.lower())\n",
    "    text = [w for w in text.split()]\n",
    "    \n",
    "    tokenized = [wordnet_lemmatizer.lemmatize(w) for w in text]\n",
    "    #return text\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'what',\n",
       " 'will',\n",
       " 'happens']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"This :) is a <br /> test! :-) and I'm not sure what will happens</br>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "labels    = []\n",
    "lenghts   = []\n",
    "\n",
    "doc_stream = stream_docs('shuffled_movie_data.csv')\n",
    "\n",
    "for idx, review in enumerate(doc_stream):\n",
    "    list_of_words = tokenizer(review[0])\n",
    "    sentences.append(list_of_words)\n",
    "    labels.append(review[1])\n",
    "    lenghts.append(len(list_of_words))\n",
    "    sys.stdout.write('\\r{:5.2f}%'.format(100*(idx+1)/50000))\n",
    "sys.stdout.write('\\rDone     \\n\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximun number of words in a review : 2507\n"
     ]
    }
   ],
   "source": [
    "MAXLEN = max(lenghts)\n",
    "print('Maximun number of words in a review :', MAXLEN)\n",
    "\n",
    "assert len(sentences) == len(labels) == 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN LEN =  236\n",
      "STD  LEN =  174.83702022169103\n"
     ]
    }
   ],
   "source": [
    "MEAN_LEN = int(sum(lenghts)/len(lenghts))\n",
    "STD_LEN  = (sum((x - MEAN_LEN)**2 for x in lenghts)/ len(lenghts))**0.5\n",
    "print('MEAN LEN = ', MEAN_LEN)\n",
    "print('STD  LEN = ', STD_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def bag_words(reviews, vocabulary):\n",
    "    all_words = []\n",
    "    for review in reviews:\n",
    "        all_words += review\n",
    "    \n",
    "    count  = [('UNKNOWN', -1)]\n",
    "    count += Counter(all_words).most_common(vocabulary - 1)\n",
    "    \n",
    "    word_dict = {}\n",
    "    for i in range(len(count)):\n",
    "        word_dict[count[i][0]] = i\n",
    "    \n",
    "    return word_dict, dict(zip(word_dict.values(), word_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_sentences(reviews, dictionary, MEAN_LEN):\n",
    "    ID_sentences = [] \n",
    "    for review in reviews:\n",
    "        ID_sentence = [0 for i in range(MEAN_LEN)]\n",
    "        for lsen, word in enumerate(review):\n",
    "            idr = dictionary.get(word, 0)\n",
    "            if lsen >= MEAN_LEN: break\n",
    "            else: ID_sentence[lsen] = idr\n",
    "        ID_sentences.append(ID_sentence)\n",
    "    return ID_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size         = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index, id_toWord = bag_words(sentences, voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN   = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDreviews = make_index_sentences(sentences, word_index, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator for training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_set(sentences, labels, lenghts, batch_size):\n",
    "    N = len(sentences)\n",
    "    for i in range(0, N, batch_size):\n",
    "        embeddings = np.array(sentences[i : i + batch_size], dtype = np.int32)\n",
    "        batch_lebl = np.reshape(np.array(labels[i: i + batch_size] , dtype = np.int32), (-1, 1))\n",
    "        seq_lenght = np.array(lenghts[i: i + batch_size], dtype = np.int32)\n",
    "        yield embeddings, batch_lebl, seq_lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_shape :  (10, 400) , label_shape:  (10, 1) , seq_shape :  (10,)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "gen = get_training_set(IDreviews, labels, lenghts, batch_size)\n",
    "batch1, batch2, batch3 = next(gen)\n",
    "print('review_shape : ', batch1.shape, ', label_shape: ', batch2.shape,', seq_shape : ', batch3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gen, batch1, batch2, batch3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recuerrent neural network about two layers of bidirectional LSTMs with dropout, and also two additional dense layers on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "def getWeights(shape):\n",
    "    initVar = weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "    return tf.get_variable('W',\n",
    "                            dtype = tf.float32,\n",
    "                            shape = shape,\n",
    "                            initializer = tf.truncated_normal_initializer(stddev=0.01))\n",
    "\n",
    "def getBiases(shape):\n",
    "    initVar = tf.constant(0.0, shape = shape, dtype = tf.float32)\n",
    "    return tf.get_variable('b',\n",
    "                            dtype = tf.float32,\n",
    "                            initializer = initVar)\n",
    "\n",
    "def RNN(input_rev, vocabulary_size, emb_size, n_hidden, n_dense, batch_size, seq_max_len, seq_len, num_layers, prob):  \n",
    "    embedding = tf.Variable(tf.random_uniform((vocabulary_size, emb_size), -1, 1))\n",
    "    embed     = tf.nn.embedding_lookup(embedding, input_rev)\n",
    "    \n",
    "    lstms_fw = [tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(n_hidden) for _ in range(num_layers)]\n",
    "    cell_fw  = tf.contrib.rnn.MultiRNNCell(lstms_fw)\n",
    "\n",
    "    lstms_bw = [tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(n_hidden) for _ in range(num_layers)]\n",
    "    cell_bw  = tf.contrib.rnn.MultiRNNCell(lstms_bw)\n",
    "    \n",
    "    cell_fw  = tf.nn.rnn_cell.DropoutWrapper(cell_fw, output_keep_prob=prob)\n",
    "    cell_bw  = tf.nn.rnn_cell.DropoutWrapper(cell_bw, output_keep_prob=prob)\n",
    "\n",
    "    (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                      cell_bw,\n",
    "                                                      embed,\n",
    "                                                      sequence_length=seq_len,\n",
    "                                                      dtype=tf.float32)\n",
    "    outputs = tf.concat([output_fw, output_bw], axis=2)\n",
    "\n",
    "    index   = tf.range(0, batch_size) * seq_max_len + (seq_len - 1)\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden*2]), index)\n",
    "    out     = tf.layers.dense(inputs=outputs, units=1)\n",
    "    res     = tf.sigmoid(out, 'sigmoid')\n",
    "    #return res   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size         = 32\n",
    "num_hidden_units = 16\n",
    "num_dense_units  = 10\n",
    "number_of_layers = 2\n",
    "seq_max_len      = MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate  = 0.01\n",
    "batch_size     = 300\n",
    "training_steps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 400, 64)\n"
     ]
    }
   ],
   "source": [
    "#X       = tf.placeholder(tf.float32, [None, seq_max_len, input_dim], name = 'input')\n",
    "X       = tf.placeholder(tf.int32,   [batch_size, seq_max_len], name='input')\n",
    "seqLen  = tf.placeholder(tf.int32  , [batch_size], name = 'seq_len')\n",
    "y       = tf.placeholder(tf.int32,   [batch_size, 1], name = 'labels')\n",
    "tprob   = tf.placeholder_with_default(1.0, shape=())\n",
    "with tf.variable_scope(\"RNN\", reuse=tf.AUTO_REUSE):\n",
    "    pred_out = RNN(X, voc_size, emb_size, num_hidden_units, num_dense_units, batch_size, seq_max_len, seqLen, number_of_layers, tprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Train\", reuse=tf.AUTO_REUSE):\n",
    "    #cost       = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.cast(y, dtype=tf.float32), logits = pred_out))\n",
    "    assert y.shape[1] == pred_out.shape[1]\n",
    "    \n",
    "    cost       = tf.losses.mean_squared_error(y, pred_out)\n",
    "    train_op   = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    pred_class = tf.greater(pred_out,0.5)\n",
    "    acc_mes    = tf.equal(pred_class, tf.equal(y,1), name = 'correct_pred')\n",
    "    acc        = tf.reduce_mean(tf.cast(acc_mes, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size =  45000 , Test Size =  5000\n"
     ]
    }
   ],
   "source": [
    "init     = tf.global_variables_initializer()\n",
    "\n",
    "N = len(sentences)\n",
    "\n",
    "cross_val = 0.9\n",
    "cross_div = int(N * cross_val)\n",
    "\n",
    "X_train = IDreviews[:cross_div]\n",
    "y_train = labels[:cross_div]\n",
    "l_train = lenghts[:cross_div]\n",
    "\n",
    "X_test  = IDreviews[cross_div:]\n",
    "y_test  = labels[cross_div:]\n",
    "l_test  = lenghts[cross_div:]\n",
    "\n",
    "assert len(X_train) == len(y_train) == len(l_train) == cross_div\n",
    "assert len(X_test)  == len(y_test)  == len(l_test)  == N - cross_div\n",
    "\n",
    "print('Train Size = ', len(X_train), ', Test Size = ', len(X_test))\n",
    "train_loss = []\n",
    "test_acc   = []\n",
    "test_loss  = []\n",
    "\n",
    "fmt = 'epoch : {:4d}, train loss = {:4.3f}, test loss = {:4.3f}, test accuracy = {:4.3f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-53897f84b456>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mx_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m     \u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m     \u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqLen\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_len_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtprob\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    for ep in range(1, training_steps + 1):\n",
    "        gen = get_training_set(X_train, y_train, l_train, batch_size)\n",
    "        loss_t = []\n",
    "        for i in range(1, len(X_train) // batch_size + 1):\n",
    "            x_batch , y_batch, seq_len_batch = next(gen)\n",
    "            _, loss = sess.run([train_op, cost], feed_dict={X     : x_batch, y     : y_batch, seqLen: seq_len_batch,tprob : 0.5})\n",
    "            loss_t.append(loss)\n",
    "\n",
    "        train_loss.append(sum(loss_t)/len(loss_t))\n",
    "\n",
    "        gen_test = get_training_set(X_test, y_test, l_test, batch_size)\n",
    "        acc_t    = []\n",
    "        loss_test= []    \n",
    "        for i in range(len(X_test) // batch_size):\n",
    "            x_batch , y_batch, seq_len_batch = next(gen_test)\n",
    "            accuracy, loss = sess.run([acc, cost], feed_dict={X     : x_batch, \n",
    "                                                              y     : y_batch, \n",
    "                                                              seqLen: seq_len_batch,\n",
    "                                                              tprob : 1.0})\n",
    "            acc_t.append(accuracy)\n",
    "            loss_test.append(loss)\n",
    "        test_acc.append(sum(acc_t)/len(acc_t))\n",
    "        test_loss.append(sum(loss_test)/len(loss_test))\n",
    "\n",
    "        print(fmt.format(ep, train_loss[-1], test_loss[-1], test_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This lines are for plot error on each epoch\n",
    "\n",
    "ep  = np.arange(1, training_steps + 1, 1)\n",
    "fig, ax   = plt.subplots(figsize=(14, 8))\n",
    "l1, l2    = ax.plot(ep, train_loss, ep, test_loss)\n",
    "ax.set(xlabel='Epoch', ylabel='Cost', title='Recurrent Neural Network - Training Loss')\n",
    "ax.axis([0.0, training_steps + 0.5, min(train_loss)-0.01, train_loss[0]+0.01])\n",
    "plt.legend([l1, l2],[\"Training\",\"Testing\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "l2      = ax.plot( ep, test_acc)\n",
    "ax.set(xlabel='Epoch', ylabel='Cost', title='Recurrent Neural Network - Testing Accuracy ')\n",
    "ax.axis([0.8, training_steps + 0.5, test_acc[0]-0.01, max(test_acc)+0.01])\n",
    "#plt.legend([l1, l2],[\"Training\",\"Validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
